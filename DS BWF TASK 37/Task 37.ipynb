{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.7.24-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\abuba\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 670.4 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 670.4 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 670.4 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 670.4 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 670.4 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 385.8 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 385.8 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 457.5 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 520.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 556.5 kB/s eta 0:00:00\n",
      "Downloading regex-2024.7.24-cp312-cp312-win_amd64.whl (269 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.7.24 tqdm-4.66.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\abuba\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\abuba\\AppData\\Local\\Programs\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to abubakrdanish/-Data-\n",
      "[nltk_data]     Science-Group-2---BWF---Abubakar-Danish/DS BWF TASK\n",
      "[nltk_data]     37/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to abubakrdanish/-Data-\n",
      "[nltk_data]     Science-Group-2---BWF---Abubakar-Danish/DS BWF TASK\n",
      "[nltk_data]     37/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to abubakrdanish/-Data-\n",
      "[nltk_data]     Science-Group-2---BWF---Abubakar-Danish/DS BWF TASK\n",
      "[nltk_data]     37/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text Data:\n",
      "['Natural Language Processing is an interesting field.', 'This is an example of preprocessing text.']\n",
      "\n",
      "Processed Text Data:\n",
      "['natural language processing interesting field', 'example preprocessing text']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.data.path.append('abubakrdanish/-Data-Science-Group-2---BWF---Abubakar-Danish/DS BWF TASK 37/nltk_data')\n",
    "\n",
    "\n",
    "nltk.download('wordnet', download_dir='abubakrdanish/-Data-Science-Group-2---BWF---Abubakar-Danish/DS BWF TASK 37/nltk_data')\n",
    "nltk.download('omw-1.4', download_dir='abubakrdanish/-Data-Science-Group-2---BWF---Abubakar-Danish/DS BWF TASK 37/nltk_data')\n",
    "nltk.download('stopwords', download_dir='abubakrdanish/-Data-Science-Group-2---BWF---Abubakar-Danish/DS BWF TASK 37/nltk_data')\n",
    "\n",
    "# Sample text data\n",
    "text_data = [\n",
    "    \"Natural Language Processing is an interesting field.\",\n",
    "    \"This is an example of preprocessing text.\"\n",
    "]\n",
    "\n",
    "# 1. Lowercase the text\n",
    "text_data_lower = [text.lower() for text in text_data]\n",
    "\n",
    "# 2. Tokenization (simple split)\n",
    "text_data_tokens = [text.split() for text in text_data_lower]\n",
    "\n",
    "# 3. Removing special characters\n",
    "text_data_no_special_chars = [[re.sub(r'\\W+', '', word) for word in text] for text in text_data_tokens]\n",
    "\n",
    "# 4. Removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "text_data_no_stopwords = [[word for word in text if word not in stop_words] for text in text_data_no_special_chars]\n",
    "\n",
    "# 5. Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "text_data_lemmatized = [[lemmatizer.lemmatize(word) for word in text] for text in text_data_no_stopwords]\n",
    "\n",
    "# 6. Removing Numbers\n",
    "text_data_no_numbers = [[re.sub(r'\\d+', '', word) for word in text] for text in text_data_lemmatized]\n",
    "\n",
    "# 7. Rejoin tokens into a single string\n",
    "processed_text_data = [' '.join(text) for text in text_data_no_numbers]\n",
    "\n",
    "# Print the processed text data\n",
    "print(\"Original Text Data:\")\n",
    "print(text_data)\n",
    "\n",
    "print(\"\\nProcessed Text Data:\")\n",
    "print(processed_text_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
